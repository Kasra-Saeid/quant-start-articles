
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="google-site-verification" content="wl3-8ed1QZjI0iYZMv10zoZWYElkMObTfwLlWIj9cpA" />
    <meta name="description" content="Beginner&#x27;s Guide to Decision Trees for Supervised Machine Learning">

    <link rel="icon" href="/static/images/favicon.png">

    <title>Beginner&#x27;s Guide to Decision Trees for Supervised Machine Learning | QuantStart</title>
    
    
<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900&display=swap" rel="stylesheet"> 
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700,900&display=swap" rel="stylesheet"> 
<link href="/static/css/bootstrap.min.css" rel="stylesheet">
<link href="/static/css/prism.css" rel="stylesheet">
<link href="/static/css/qs.css?v=10" rel="stylesheet">
    
  </head>

  <body>
    <header class="header covered-header" style="background-image: linear-gradient(rgba(0, 0, 0, 0.8), rgba(0, 0, 0, 0.8)), url(https://quantstartmedia.s3.amazonaws.com/images/article-images/article-backgrounds/default-bg.jpg);">
  
<nav class="nav">
  <div class="container nav-container">
    <div class="nav-row row d-flex justify-content-between align-items-center">
      <div class="col-2">
        <ul class="nav-items justify-content-end small-capitals align-items-center">
          <li class="nav-item">
            <a class="link-fade" href="/">QuantStart</a>
          </li>
        </ul>
      </div>
      <div class="col-auto col-logo">
        <ul id="top-nav-menu" class="nav-items justify-content-end align-items-center">
          
          <li class="nav-item">
            <a class="link-fade" href="/qsalpha/">QSAlpha</a>
          </li>
          
          
          <li class="nav-item">
            <a class="link-fade" href="/quantcademy/">Quantcademy</a>
          </li>
          
          <li id="menu-link-ebooks" class="nav-item">
            <a class="link-fade" href="#">Books</a>
            <div id="menu-pane-ebooks" class="nav-items menu-dropdown-pane">
              <div class="nav-item">
                <a class="link-fade d-block ml-3 mr-3 my-3 mt-4" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a>
              </div>
              <div class="nav-item">
                <a class="link-fade d-block ml-3 mr-3 my-3 mt-4" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a>
              </div>
              <div class="nav-item">
                <a class="link-fade d-block ml-3 mr-3 my-3 mt-4" href="/cpp-for-quantitative-finance-ebook/">C++ For Quantitative Finance</a>
              </div>
            </div>
          </li>
          <li class="nav-item">
            <a class="link-fade" href="/qstrader/">QSTrader</a>
          </li>
          <li class="nav-item">
            <a class="link-fade" href="/articles/">Articles</a>
          </li>
          
          <li class="nav-item">
            <a class="link-fade" href="/members/login/">Login</a>
          </li>
          
        </ul>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
      </div>
    </div>
  </div>
</nav>

<nav id="mobile-nav" class="mobile-nav text-left">
  <div class="container">
    <ul class="mt-4 ml-3 mobile-nav-menu">
      <li class="nav-item">
        <a class="link-fade d-block" href="/">QuantStart</a>
      </li>

      
      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/qsalpha/">QSAlpha</a>
      </li>
      

      
      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/quantcademy/">Quantcademy</a>
      </li>
      

      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="#">Books</a>
      </li>
      <li class="nav-item sub-item">
        <a class="link-fade d-block ml-3" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a>
      </li>
      <li class="nav-item sub-item">
        <a class="link-fade d-block ml-3" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a>
      </li>
      <li class="nav-item sub-item">
        <a class="link-fade d-block ml-3" href="/cpp-for-quantitative-finance-ebook/">C++ For Quantitative Finance</a>
      </li>

      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/qstrader/">QSTrader</a>
      </li>

      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/articles/">Articles</a>
      </li>

      
      <li class="nav-item">
        <a class="link-fade d-block pt-3" href="/members/login/">Login</a>
      </li>
      
    </ul>
    <button class="nav-toggle mobile-nav-close">
      <svg id="mobile-nav-close-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
        <path d="M19.77,5.63,13.41,12l6.36,6.37a1,1,0,0,1-1.41,1.41L12,13.41,5.63,19.77a1,1,0,0,1-1.44-1.39l0,0L10.58,12,4.21,5.63a1,1,0,0,1,0-1.42,1,1,0,0,1,1.41,0l0,0L12,10.58l6.37-6.37a1,1,0,0,1,1.41,0A1,1,0,0,1,19.77,5.63Z"></path>
      </svg>
    </button>
  </div>
</nav>

  <div class="container hero-container">
    <section class="mt-5 mb-4">
      <div class="row justify-content-center">
        <div class="col-12 text-center">
          <p class="hero">Beginner&#x27;s Guide to Decision Trees for Supervised Machine Learning</p>
          <p class="hero subhero">Beginner&#x27;s Guide to Decision Trees for Supervised Machine Learning</p>
        </div>
      </div>
    </section>
  </div>
</header>
    
<section class="container content-container">
  <div class="row">
    <div class="col-md-8 order-md-2">
      <section class="content article-content">
        
        
        <p>In this article we are going to consider a stastical machine learning method known as a <strong>Decision Tree</strong>. Decision Trees (DTs) are a supervised learning technique that predict values of <em>responses</em> by learning decision rules derived from <em>features</em>. They can be used in both a <a href="http://en.wikipedia.org/wiki/Regression_analysis">regression</a> and a <a href="http://en.wikipedia.org/wiki/Statistical_classification">classification</a> context. For this reason they are sometimes also referred to as Classification And Regression Trees (CART).</p>

<p>DT/CART models are an example of a more general area of machine learning known as <strong>adaptive basis function models</strong>. These models learn the features directly from the data, rather than being prespecified, as in some other basis expansions. However, unlike linear regression, these models are not <em>linear in the parameters</em> and so we are only able to compute a <em>locally</em> optimal maximum likelihood estimate (MLE) for the parameters<sup><a href="#ref-murphy">[1]</a></sup>.</p>

<p>DT/CART models work by partitioning the feature space into a number of simple rectangular regions, divided up by <em>axis parallel splits</em>. In order to obtain a prediction for a particular observation, the <a href="http://en.wikipedia.org/wiki/Mean">mean</a> or <a href="http://en.wikipedia.org/wiki/Mode_%28statistics%29">mode</a> of the <em>training observations'</em> responses, within the partition that the new observation belongs to, is used.</p>

<p>One of the primary benefits of using a DT/CART is that, by construction, it produces interpretable <em>if-then-else</em> decision rulesets, which are akin to graphical flowcharts.</p>

<p>Their main disadvantage lies in the fact that they are often uncompetitive with other supervised techniques such as <a href="http://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners">support vector machines</a> or deep neural networks in terms of prediction accuracy.</p>

<p>However they can become extremely competitive when used in an <em>ensemble</em> method such as with bootstrap aggregation (<strong>"bagging"</strong>), <strong>Random Forests</strong> or <strong>boosting</strong>.</p>

<p>In quantitative finance ensembles of DT/CART models are used in forecasting, either future asset prices/directions or liquidity of certain instruments. In future articles we will build trading strategies based off these methods.</p>

<h2>Mathematical Overview</h2>

<p>Under a probabilistic adaptive basis function specification the model $f({\bf x})$ is given by<sup><a href="#ref-murphy">[1]</a></sup>:</p>

<p>
\begin{eqnarray}
f({\bf x}) = \mathbb{E}(y \mid {\bf x}) = \sum^{M}_{m=1} w_m \phi({\bf x}; {\bf v}_m)
\end{eqnarray}
</p>

<p>Where $w_m$ is the mean response in a particular region, $R_m$, and ${\bf v}_m$ represents how each variable is split at a particular threshold value. These splits define how the feature space in $R^p$ into $M$ separate "hyperblock" regions.</p>

<h2>Decision Trees for Regression</h2>

<p>Let us consider an abstract example of regression problem with two feature variables ($X_1$, $X_2$) and a numerical response $y$. This will allow us to easily visualise the nature of partitioning carried out by the tree.</p>

<p>In the following figure we can see a pre-grown tree for this particular example:</p>

<p style="text-align:center;">
<img width="70%" src="https://s3.amazonaws.com/quantstartmedia/images/qs-cart-example-tree.png"><br><br><strong>A Decision Tree with six separate regions</strong>
</p>

<p>How does this correspond to a partitioning of the feature space? The following figure depicts a subset of $\mathbb{R}^2$ that contains our example data. Notice how the domain is partitioned using axis-parallel splits. That is, every split of the domain is aligned with one of the feature axes:</p>

<p style="text-align:center;">
<img width="70%" src="https://s3.amazonaws.com/quantstartmedia/images/qs-cart-axis-parallel-split.png"><br><br><strong>The resulting partition of the subset of $\mathbb{R}^2$ into six regional "blocks"</strong>
</p>

<p>The concept of axis parallel splitting generalises straightforwardly to dimensions greater than two. For a feature space of size $p$, a subset of $\mathbb{R}^p$, the space is divided into $M$ regions, $R_m$, each of which is a $p$-dimensional "hyperblock".</p>

<p>We have yet to discuss <em>how</em> such a tree is "grown" or "trained". The following section outlines the algorithm for carrying this out.</p>

<h3>Creating a Regression Tree and Making Predictions</h3>

<p>The basic heuristic for creating a DT is as follows:</p>

<ul>
  <li>Given $p$ features, partition the p-dimensional feature space (a subset of $\mathbb{R}^p$) into $M$ mutually distinct regions that fully cover the subset of feature space and do not overlap. These regions are given by $R_1,...,R_M$.</li>
  <li>Any new observation that falls into a particular partition $R_m$ has the estimated response given by the mean of all <em>training observations</em> with the partition, denoted by $w_m$.</li>
</ul>

<p>However, this process doesn't actually describe <em>how</em> to form the partition in an algorithmic manner! For that we need to use a technique known as <strong>Recursive Binary Splitting</strong> (RBS)<sup><a href="#ref-isl">[2]</a></sup>.</p>

<h4>Recursive Binary Splitting</h4>

<p>Our goal for this algorithm is to minimise some form of error criterion. In this particular instance we wish to minimise the <strong>Residual Sum of Squares</strong> (RSS), an error measure also used in linear regression settings. The RSS, in the case of a partitioned feature space with $M$ partitions is given by:</p>

\begin{eqnarray}
\text{RSS} = \sum^{M}_{m=1} \sum_{i \in R_m} ( y_i - \hat{y}_{R_m} )^2
\end{eqnarray}

<p>First we sum across all of the partitions of the feature space (the first summation sign) and then we sum across all test observations (indexed by $i$) in a particular partition (the second summation sign). We then take the squared difference of the response $y_i$ of a particular testing observation with the mean response $\hat{y}_{R_m}$ of the training observations within partition $m$.</p>

<p>Unfortunately it is too computationally expensive to consider all possible partitions of the feature space into $M$ rectangles (in fact the problem is <a href="https://en.wikipedia.org/wiki/NP-completeness">NP-complete</a>). Hence we must use a less computationally intensive, but more sophisticated search approach. This is where RBS comes in.</p>

<p>RBS approaches the problem by beginning at the top of the tree and splitting the tree into two branches, which creates a partition of two spaces. It carries out this particular split at the top of the tree multiple times and chooses the split of the features that minimises the (current) RSS.</p>

<p>At this point the tree creates a new branch in a particular partition and carries out the same procedure, that is, evaluates the RSS at each split of the partition and chooses the best.</p>

<p>This makes it a <strong>greedy</strong> algorithm, meaning that it carries out the evaluation for each iteration of the recursion, rather than "looking ahead" and continuing to branch before making the evaluations. It is this "greedy" nature of the algorithm that makes it computationally feasible and thus practical for use<sup><a href="#ref-murphy">[1]</a>, <a href="#ref-isl">[2]</a></sup>.</p>

<p>At this stage we haven't outlined when this procedure actually terminates. There are a few criteria that we could consider, including limiting the maximum depth of the tree, ensuring sufficient training examples in each region and/or ensuring that the regions are sufficiently homogeneous such that the tree is relatively "balanced".</p>

<p>However, as with all supervised machine learning methods, we need to constantly be aware of overfitting. This motivates the concept of "pruning" the tree.</p>

<h3>Pruning The Tree</h3>

<p>Because of the ever-present worry of overfitting and the <a href="https://www.quantstart.com/articles/The-Bias-Variance-Tradeoff-in-Statistical-Machine-Learning-The-Regression-Setting">bias-variance tradeoff</a> we need a means of adjusting the tree splitting process such that it can generalise well to test sets.</p>

<p>Since it is too costly to use <a href="https://www.quantstart.com/articles/Using-Cross-Validation-to-Optimise-a-Machine-Learning-Method-The-Regression-Setting">cross-validation</a> directly on every possible sub-tree combination while growing the tree, we need an alternative approach that still provides a good test error rate.</p>

<p>The usual approach is to grow the full tree to a prespecified depth and then carry out a procedure known as "pruning". One approach is called <em>cost-complexity pruning</em> and is described in detail in <a href="#ref-isl">[2]</a> and <a href="#ref-esl">[3]</a>. The basic idea is to introduce an additional tuning parameter, denoted by $\alpha$ that balances the depth of the tree and its goodness of fit to the training data. The approach used is similar to the <a href="http://www.jstor.org/stable/2346178">LASSO technique</a> developed by Tibshirani.</p>

<p>The details of the tree pruning will not concern us here as we can make use of Scikit-Learn to help us with this aspect.</p>

<h2>Decision Trees for Classification</h2>

<p>In this article we have concentrated almost exclusively on the regression case, but decision trees work equally well for classification, hence the "C" in CART models!</p>

<p>The only difference, as with all classification regimes, is that we are now predicting a categorical, rather than continuous, response value. In order to actually make a <em>prediction</em> for a categorical class we have to instead use the <em>mode</em> of the training region to which an observation belongs, rather than the <em>mean</em> value. That is, we take the most commonly occurring class value and assign it as the response of the observation.</p>

<p>In addition we need to consider alternative criteria for splitting the trees as the usual RSS score isn't applicable in the categorical setting. There are three that we will consider, which include the "hit rate", the Gini Index and Cross-Entropy<sup><a href="#ref-murphy">[1]</a>, <a href="#ref-isl">[2]</a>, <a href="#ref-esl">[3]</a></sup>.</p>

<h3>Classification Error Rate/Hit Rate</h3>

<p>Rather than seeing how far a numerical response is away from the mean value, as in the regression setting, we can instead define the "hit rate" as the fraction of training observations in a particular region that don't belong to the most widely occuring class. That is, the error is given by<sup><a href="#ref-murphy">[1]</a>, <a href="#ref-isl">[2]</a></sup>:</p>

<p>
\begin{eqnarray}
E = 1 - \text{argmax}_{c} (\hat{\pi}_{mc})
\end{eqnarray}
</p>

<p>Where $\hat{\pi}_{mc}$ represents the fraction of training data in region $R_m$ that belong to class $c$.</p>

<h3>Gini Index</h3>

<p>The <em>Gini Index</em> is an alternative error metric that is designed to show how "pure" a region is. "Purity" in this case means how much of the training data in a particular region belongs to a single class. If a region $R_m$ contains data that is mostly from a single class $c$ then the Gini Index value will be small:</p>

<p>
\begin{eqnarray}
G = \sum_{c=1}^C \hat{\pi}_{mc} (1 - \hat{\pi}_{mc})
\end{eqnarray}
</p>

<h3>Cross-Entropy/Deviance</h3>

<p>A third alternative, which is similar to the Gini Index, is known as the <em>Cross-Entropy</em> or <em>Deviance</em>:</p>

<p>
\begin{eqnarray}
D = - \sum_{c=1}^C \hat{\pi}_{mc} \text{log} \hat{\pi}_{mc}
\end{eqnarray}
</p>

<p>This motivates the question as to which error metric to use when growing a classification tree. I will state here that the Gini Index and Deviance are used more often than the Hit Rate, in order to maximise for prediction accuracy. We won't dwell on the reasons for this, but a good discussion can be found in the books provided in the References section below.</p>

<p>In future articles we will utilise the Scikit-Learn library to perform classification tasks and assess these error measures in order to determine how effective our predictions are on unseen data.</p>

<h2>Advantages and Disadvantages of Decision Trees</h2>

<p>As with all machine learning methods there are pros and cons to using DT/CARTs over other models:</p>

<h3>Advantages</h3>

<ul>
  <li>DT/CART models are easy to interpret, as "if-else" rules</li>
  <li>The models can handle categorical and continuous features in the same data set</li>
  <li>The method of construction for DT/CART models means that feature variables are automatically selected, rather than having to use subset selection or similar</li>
  <li>The models are able to scale effectively on large datasets</li>
</ul>

<h3>Disadvantages</h3>

<ul>
  <li>Poor relative prediction performance compared to other ML models</li>
  <li>DT/CART models suffer from <em>instability</em>, which means they are very sensitive to small changes in the feature space. In the language of the bias-variance trade-off, they are high variance estimators.</li>
</ul>

<p>While DT/CART models themselves suffer from poor prediction performance they are extremely competitive when utilised in an <em>ensemble</em> setting, via bootstrap aggregation ("bagging"), Random Forests or boosting.</p>

<p>In subsequent articles we will use the <a href="http://scikit-learn.org/stable/modules/tree.html">Decision Tree module</a> of the Python <a href="http://scikit-learn.org/stable">scikit-learn</a> library for classification and regression purposes on some quant finance datasets.</p>

<p>In addition we will show how ensembles of DT/CART models can perform extremely well for certain quant finance datasets.</p>

<h2>Bibliographic Note</h2>

<p>A gentle introduction to tree-based methods can be found in <a href="#ref-isl">James et al (2013)</a>, which covers the basics of both DTs and their associated ensemble methods. A more rigourous account, pitched at the late undergraduate/early graduate mathematics/statistics level can be found in <a href="#ref-esl">Hastie et al (2009)</a>. <a href="#ref-murphy">Murphy (2012)</a> provides a discussion Adaptive Basis Function Models, of which DT/CART models are a subset. The book covers both the frequentist and Bayesian approach to these models. For the practitioner working on "real world" data (such as quants like us!), <a href="#ref-kuhn">Kuhn et al (2013)</a> is appropriate text pitched at a simpler level.</p>

<h2>References</h2>

<ul>
  <li><a name="ref-murphy" href="https://www.cs.ubc.ca/~murphyk/MLbook/">[1] Murphy, K.P. (2012) <em>Machine Learning A Probabilistic Perspective</em>, MIT Press</a></li>
  <li><a name="ref-isl" href="http://www-bcf.usc.edu/~gareth/ISL/">[2] James, G., Witten, D., Hastie, T., Tibshirani, R. (2013) <em>An Introduction to Statistical Learning</em>, Springer</a></li>
  <li><a name="ref-esl" href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">[3] Hastie, T., Tibshirani, R., Friedman, J. (2009) <em>The Elements of Statistical Learning</em>, Springer</a></li>
  <li><a name="ref-kuhn" href="http://appliedpredictivemodeling.com/">[4] Kuhn, M., Johnson, K. (2013) <em>Applied Predictive Modeling</em>, Springer</a></li>
</ul>
        
        
        <script async data-uid="6296c27f4b" src="https://weathered-brook-5281.ck.page/6296c27f4b/index.js"></script>
      </section>
    </div>
    <div class="col-md-4 book-card order-md-1">
      
<div class="sidebar-advert-container">
  <a href="/qsalpha/?ref=art">
    <img class="card-img-top" src="/static/images/qsalpha-sidebar-advert-small.png" alt="QSAlpha">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/qsalpha/?ref=art">QSAlpha</a></h3>
      <p class="card-text medium-text mb-3">Join the QSAlpha research platform that helps fill your strategy research pipeline, diversifies your portfolio and improves your risk-adjusted returns for increased profitability.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/qsalpha/?ref=art">Find Out More</a>
        </div>
      </div>
    </div>
  </div>

  <a href="/quantcademy/?ref=art">
    <img class="card-img-top" src="/static/images/quantcademy-sidebar-advert-small.png" alt="Quantcademy">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/quantcademy/?ref=art">The Quantcademy</a></h3>
      <p class="card-text medium-text mb-3">Join the Quantcademy membership portal that caters to the rapidly-growing retail quant trader community and learn how to increase your strategy profitability.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/quantcademy/?ref=art">Find Out More</a>
        </div>
      </div>
    </div>
  </div>

  <a href="/successful-algorithmic-trading-ebook/">
    <img class="card-img-top" src="/static/images/sat-sidebar-advert-small.png" alt="Successful Algorithmic Trading">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a></h3>
      <p class="card-text medium-text mb-3">How to find new trading strategy ideas and objectively assess them for your portfolio using a Python-based backtesting engine.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/successful-algorithmic-trading-ebook/">Find Out More</a>
        </div>
      </div>
    </div>
  </div>

  <a href="/advanced-algorithmic-trading-ebook/">
    <img class="card-img-top" src="/static/images/aat-sidebar-advert-small.png" alt="Advanced Algorithmic Trading">
  </a>
  <div class="card mb-4 box-shadow">
    <div class="card-body text-center">
      <h3 class="mb-3"><a class="link-fade" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a></h3>
      <p class="card-text medium-text mb-3">How to implement advanced trading strategies using time series analysis, machine learning and Bayesian statistics with R and Python.</p>
      <div class="d-flex justify-content-center align-items-center">
        <div class="btn-group">
          <a class="btn btn-padded btn-outline-primary btn-lg-cta" href="/advanced-algorithmic-trading-ebook/">Find Out More</a>
        </div>
      </div>
    </div>
  </div>
</div>
    </div>
  </div>
</section>

    

<footer>
  <div class="container container-full">
    <section class="mt-1.5 mb-1">
      <div class="row">
        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">QuantStart</li>
            <li class="footer-list-link"><a class="link-fade" href="/about/">About</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/articles/">Articles</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/sitemap/">Sitemap</a></li>
          </ul>
        </div>

        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">Products</li>
            
            <li class="footer-list-link"><a class="link-fade" href="/qsalpha/">QSAlpha</a></li>
            
            
            <li class="footer-list-link"><a class="link-fade" href="/quantcademy/">Quantcademy</a></li>
            
            <li class="footer-list-link"><a class="link-fade" href="/qstrader/">QSTrader</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/successful-algorithmic-trading-ebook/">Successful Algorithmic Trading</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/advanced-algorithmic-trading-ebook/">Advanced Algorithmic Trading</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/cpp-for-quantitative-finance-ebook/">C++ For Quantitative Finance</a></li>
          </ul>
        </div>

        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">Legal</li>
            <li class="footer-list-link"><a class="link-fade" href="/privacy-policy/">Privacy Policy</a></li>
            <li class="footer-list-link"><a class="link-fade" href="/terms-and-conditions/">Terms &amp; Conditions</a></li>
          </ul>
        </div>

        <div class="col-12 col-sm-12 col-md-6 col-xl-3 mb-1.5">
          <ul class="footer-list">
            <li class="footer-list-title">Social</li>
            <li class="footer-list-link"><a class="link-fade" href="https://twitter.com/quantstart">Twitter</a></li>
            <li class="footer-list-link"><a class="link-fade" href="https://www.youtube.com/channel/UCmVnnZ6Y2TrJtY1eQJN6kWA">YouTube</a></li>
          </ul>
        </div>
      </div>
    </section>

    <div class="row mb-1.5 mt-5">
      <div class="col-12 col-md-9 col-lg-8 col-xl-6">
        <div class="footer-copyright">
          <p>Â©2012-2023 QuarkGluon Ltd. All rights reserved.</p>
        </div>
      </div>
    </div>
  </div>
</footer>

    
<script src="/static/js/jquery-3.2.1.min.js"></script>
<script src="/static/js/prism.js.min"></script>
<script src="/static/js/bootstrap.min.js"></script>
<script src="/static/js/nav.js"></script>

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-5383959-5']);
  _gaq.push(['_trackPageview']);

  (function() {
  var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>


    
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/static/js/highcharts/highcharts.js"></script>
<script type="text/javascript">
  num_colours = 10;
</script>
<script src="/static/js/statistics.js"></script>


  </body>
</html>
